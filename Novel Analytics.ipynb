{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aadbaa4-bac5-4efe-9ada-50e2de74b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading NLTK\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from pprint import pp\n",
    "# importing pandas\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b1a61b-12e4-4840-974b-73049cf45262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Content</th>\n",
       "      <th>No of Sentences</th>\n",
       "      <th>No of Words</th>\n",
       "      <th>Avg no of words/Sent</th>\n",
       "      <th>POS counts</th>\n",
       "      <th>Sentiment polarity</th>\n",
       "      <th>Sentiment subjectivity</th>\n",
       "      <th>Sentiment scores</th>\n",
       "      <th>Most Frequent 50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Location, Content, No of Sentences, No of Words, Avg no of words/Sent, POS counts, Sentiment polarity, Sentiment subjectivity, Sentiment scores, Most Frequent 50]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Location': [],\n",
    "        'Content': [], 'No of Sentences': [], 'No of Words' : [], 'Avg no of words/Sent': [], 'POS counts' : [], 'Sentiment polarity' : [], \n",
    "        'Sentiment subjectivity' : [], 'Sentiment scores' :[], 'Most Frequent 50': []}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8dbc5d6-f7dc-4c17-a637-13a671f08c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6eae5b5-23bf-402d-8659-e4e9baea5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pass_Location(location):\n",
    "    print(location)\n",
    "    global lines\n",
    "    with open(location, mode = \"r\", encoding=\"utf8\") as f:\n",
    "            lines = f.read().replace('\\n', ' ')\n",
    "            f.close()\n",
    "    #print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abac5e10-5c55-408a-a00b-6fb6a3f351d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'same', \"she's\", 'again', 'but', 'while', 'by', 'isn', 'after', 'itself', 'most', \"doesn't\", 'am', 'just', 'is', \"shouldn't\", 'such', 'here', 'are', 'on', 'between', 'i', \"wouldn't\", \"it's\", 'further', 'whom', 'd', 'o', 'off', 'm', 'from', 'any', 'wasn', 'at', 'didn', 're', 'how', 'and', 'were', 'mustn', 'me', 'won', 'that', 'until', 'below', 'hers', 't', 'each', 'has', 'no', 'haven', \"you'd\", 'for', 'yours', 'up', 'can', 'some', 'she', 'was', 'did', 'or', 'there', 'doesn', \"mightn't\", 'yourself', 'own', 'into', \"won't\", 'what', 'have', 'been', 've', 'having', 'in', 'shouldn', 'during', 'to', 'other', \"aren't\", 'ain', 'ma', 'this', \"isn't\", \"hasn't\", \"haven't\", 's', 'once', 'them', 'his', \"mustn't\", 'wouldn', 'under', 'him', 'being', 'it', \"you're\", 'he', 'should', 'all', 'not', 'because', 'these', 'ours', 'we', 'very', 'herself', 'through', 'her', \"needn't\", 'when', 'don', 'yourselves', 'so', 'mightn', 'our', 'over', \"didn't\", \"weren't\", 'against', 'as', 'more', 'than', 'your', \"should've\", \"that'll\", 'nor', 'll', 'my', 'an', 'y', 'had', \"couldn't\", 'shan', 'above', 'now', 'himself', 'ourselves', 'will', \"hadn't\", 'you', 'if', 'the', \"you've\", 'before', 'aren', 'doing', 'why', 'which', 'weren', 'who', 'then', 'needn', 'hasn', 'does', 'out', \"you'll\", 'myself', \"don't\", 'theirs', 'few', 'its', \"wasn't\", 'those', 'they', 'hadn', 'about', 'both', 'of', 'where', 'only', 'do', \"shan't\", 'down', 'their', 'with', 'too', 'couldn', 'themselves', 'a', 'be'}\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "global stop_words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1c8f49-446b-4581-ac84-9c4386d55603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we use POS (poarts of Speech - \n",
    "#Each word has a tag denoting its part-of-speech. For example, nn refers to a noun, while a tag that starts with vb is a verb.)\n",
    "\n",
    "def Text_POS(filtered_sent):    \n",
    "    global tagged_text\n",
    "\n",
    "    tagged_text=nltk.pos_tag(filtered_sent)\n",
    "    print(len(tagged_text))\n",
    "    print(np.shape(tagged_text))\n",
    "    #print(tagged_text)\n",
    "    global arr_2d\n",
    "    \n",
    "    arr_2d = np.array(tagged_text)\n",
    "    \n",
    "    for sublist in arr_2d:\n",
    "        sublist[0], sublist[1] = sublist[1], sublist[0]\n",
    "    print(arr_2d)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c900998-db06-4285-9ac2-997a1f33494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Victorian Age\\Agnes Grey by Anne Brontë.txt\n",
      "D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Victorian Age\\Agnes Grey by Anne Brontë.txt\n",
      "Columns of text converted to txt\n",
      "Average words per sentence: 26.502435064935064\n",
      "No of sentences 821\n",
      "No of words 38959\n",
      "<FreqDist with 5176 samples and 22924 outcomes>\n",
      "POS counts: Counter({'NN': 4689, 'IN': 3731, 'PRP': 3212, ',': 3058, 'DT': 2624, 'JJ': 2613, 'RB': 2154, 'VBD': 2041, 'CC': 1904, 'VB': 1826, 'NNP': 1531, 'PRP$': 1217, 'NNS': 1205, 'TO': 1139, '.': 944, ':': 827, 'VBN': 713, 'MD': 676, 'VBG': 616, 'VBP': 602, 'VBZ': 316, 'WRB': 186, 'CD': 171, 'WP': 161, 'WDT': 129, 'JJR': 112, 'RP': 112, 'JJS': 89, 'RBR': 75, 'PDT': 58, 'EX': 58, '(': 32, ')': 32, 'RBS': 26, 'FW': 26, 'UH': 18, \"''\": 18, 'WP$': 10, 'NNPS': 7, '$': 1})\n",
      "Sentiment polarity: 0.08855992354844243\n",
      "Sentiment subjectivity: 0.5226579751615135\n",
      "Sentiment scores: {'neg': 0.1, 'neu': 0.743, 'pos': 0.157, 'compound': 1.0}\n",
      "D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Victorian Age\\Arms and the Man by Bernard Shaw.txt\n",
      "D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Victorian Age\\Arms and the Man by Bernard Shaw.txt\n",
      "Columns of text converted to txt\n",
      "Average words per sentence: 7.348693865570883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:883: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  element = np.asarray(element)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sentences 3074\n",
      "No of words 32025\n",
      "<FreqDist with 4443 samples and 20939 outcomes>\n",
      "POS counts: Counter({'NN': 4011, '.': 3108, 'IN': 2732, 'PRP': 2628, 'DT': 2172, 'JJ': 2073, 'NNP': 1773, 'RB': 1400, ',': 1392, 'VB': 1077, 'VBZ': 1040, 'VBP': 940, 'PRP$': 800, 'CC': 766, 'NNS': 741, '(': 717, ')': 717, 'TO': 606, 'VBG': 545, 'VBD': 538, 'VBN': 412, 'MD': 360, ':': 349, 'WP': 176, 'RP': 174, 'WRB': 147, 'UH': 143, 'CD': 141, 'WDT': 64, 'JJS': 53, 'EX': 49, 'JJR': 47, 'PDT': 43, 'RBR': 32, 'NNPS': 23, 'FW': 12, 'RBS': 11, 'WP$': 6, '$': 4, '#': 1, 'POS': 1, \"''\": 1})\n",
      "Sentiment polarity: 0.08154861469996155\n",
      "Sentiment subjectivity: 0.513637303247512\n",
      "Sentiment scores: {'neg': 0.098, 'neu': 0.751, 'pos': 0.151, 'compound': 1.0}\n",
      "D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Victorian Age\\Far From Madding Crowd Thomas HArdy.txt\n",
      "D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Victorian Age\\Far From Madding Crowd Thomas HArdy.txt\n",
      "Columns of text converted to txt\n",
      "Average words per sentence: 16.11038961038961\n",
      "No of sentences 6191\n",
      "No of words 166876\n",
      "<FreqDist with 14248 samples and 101348 outcomes>\n",
      "POS counts: Counter({'NN': 23831, 'IN': 17627, 'DT': 14723, 'JJ': 11146, ',': 10509, 'PRP': 10378, 'NNP': 10201, 'VBD': 10058, 'RB': 8455, '.': 7280, 'VB': 5578, 'CC': 5570, 'NNS': 4555, 'VBN': 3691, 'TO': 3532, 'PRP$': 3449, 'VBP': 3121, 'VBG': 2974, 'VBZ': 1816, 'MD': 1815, ':': 982, 'CD': 803, 'WDT': 786, 'WRB': 738, 'WP': 689, 'RP': 629, 'JJR': 383, 'EX': 318, 'RBR': 239, 'PDT': 218, 'JJS': 202, 'FW': 141, 'UH': 136, \"''\": 96, 'RBS': 55, '(': 45, ')': 45, 'WP$': 35, 'NNPS': 24, 'POS': 3})\n",
      "Sentiment polarity: 0.06582836545443713\n",
      "Sentiment subjectivity: 0.48562252197482114\n",
      "Sentiment scores: {'neg': 0.084, 'neu': 0.805, 'pos': 0.112, 'compound': 1.0}\n",
      "D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Victorian Age\\Jane Eyre An Autobiography by Charlotte Brontë.txt\n",
      "D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Victorian Age\\Jane Eyre An Autobiography by Charlotte Brontë.txt\n",
      "Columns of text converted to txt\n",
      "Average words per sentence: 17.4123730378578\n",
      "No of sentences 6845\n",
      "No of words 224961\n",
      "<FreqDist with 16351 samples and 138249 outcomes>\n",
      "POS counts: Counter({'NN': 29888, 'IN': 20371, 'PRP': 20352, 'DT': 16162, ',': 14540, 'JJ': 13815, 'VBD': 13788, 'RB': 11369, 'NNP': 10427, 'CC': 8863, 'VB': 8713, '.': 8216, ':': 6269, 'PRP$': 6054, 'NNS': 5761, 'TO': 5093, 'VBN': 4748, 'VBP': 4525, 'MD': 3261, 'VBG': 2795, 'VBZ': 2689, 'WRB': 1096, 'WP': 1033, 'CD': 994, 'WDT': 807, 'RP': 792, 'JJR': 444, 'EX': 377, 'JJS': 306, 'RBR': 266, 'PDT': 199, 'FW': 176, 'UH': 173, \"''\": 168, '(': 140, ')': 140, 'WP$': 63, 'RBS': 57, 'NNPS': 26, 'POS': 3, '#': 1, 'SYM': 1})\n",
      "Sentiment polarity: 0.08395447577503284\n",
      "Sentiment subjectivity: 0.5022539406773097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18760\\1179905964.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Perform sentiment analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0manalyzer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;31m# Print the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36mpolarity_scores\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m             if (\n\u001b[1;32m--> 370\u001b[1;33m                 \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_and_emoticons\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"kind\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"of\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#If I get IOPub data rate exceed. Write this code in console - jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10 \n",
    "\n",
    "# list to store files\n",
    "res = []\n",
    "dir_path = r'D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Victorian Age'\n",
    "tempText = ()\n",
    "i = 0\n",
    "a = \"\"\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        location = os.path.join(dir_path, path)\n",
    "        print(location)\n",
    "                \n",
    "        #Read file from location\n",
    "        Pass_Location(location) \n",
    "        #print(lines)\n",
    "        print(\"Columns of text converted to txt\")\n",
    "        \n",
    "        # Split the text into sentences using regular expressions\n",
    "        sentences = re.split('[.!?]',lines)\n",
    "\n",
    "        # Remove any empty sentences\n",
    "        sentences = list(filter(None, sentences))\n",
    "\n",
    "        # Calculate the average number of words per sentence\n",
    "        total_words = sum(len(sentence.split()) for sentence in sentences)\n",
    "        avg_words_per_sentence = total_words / len(sentences)\n",
    "\n",
    "        # Print the result\n",
    "        print(\"Average words per sentence:\", avg_words_per_sentence)\n",
    "        \n",
    "        \n",
    "        #Tokenization Sentence & word\n",
    "        tokenized_text=sent_tokenize(lines)\n",
    "       # print(tokenized_text) - its working. Closed it to avoid excess printing\n",
    "        tokenized_word=word_tokenize(lines)\n",
    "        #print(tokenized_word) - its working. Closed it to avoid excess printing\n",
    "        \n",
    "        print(\"No of sentences\", len(tokenized_text))\n",
    "        print(\"No of words\", len(tokenized_word))\n",
    "        \n",
    "        filtered_sent=[]\n",
    "        for w in tokenized_word:\n",
    "            if w not in stop_words:\n",
    "                filtered_sent.append(w)\n",
    "        #print(\"Tokenized Sentence:\",tokenized_text)\n",
    "        #print(\"Filterd Sentence:\",filtered_sent)\n",
    "                \n",
    "        fdist = FreqDist(filtered_sent)\n",
    "        print(fdist)\n",
    "        #print(fdist.most_common(25)) - will open later when printing to file\n",
    "        \n",
    "        #Creating the summation\n",
    "        # Tokenize the text\n",
    "        tokens = nltk.word_tokenize(lines)\n",
    "\n",
    "        # Perform POS tagging\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "        # Initialize counters\n",
    "        counts = Counter(tag for word, tag in pos_tags)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"POS counts:\", counts)\n",
    "\n",
    "        #Sentiment_Analysis(lines)\n",
    "        from textblob import TextBlob\n",
    "\n",
    "        # Load the text to analyze\n",
    "        # Perform sentiment analysis\n",
    "        blob = TextBlob(lines)\n",
    "        sentiment = blob.sentiment\n",
    "\n",
    "        # Print the results\n",
    "        print(\"Sentiment polarity:\", sentiment.polarity)\n",
    "        print(\"Sentiment subjectivity:\", sentiment.subjectivity)\n",
    "        \n",
    "        \n",
    "        from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "          \n",
    "\n",
    "        # Perform sentiment analysis\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        scores = analyzer.polarity_scores(lines)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"Sentiment scores:\", scores)\n",
    "        \n",
    "               \n",
    "        df.loc[len(df.index)] = [location, lines, len(tokenized_text), len(tokenized_word), avg_words_per_sentence, counts, sentiment.polarity, sentiment.subjectivity, scores, fdist.most_common(50)]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01233a-5845-4c54-a63f-9e59e247f12c",
   "metadata": {},
   "source": [
    "Tag Descriptions\n",
    "We can describe the meaning of each tag by using the following program which shows the in-built values.\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('IN')\n",
    "nltk.help.upenn_tagset('DT')\n",
    "When we run the above program, we get the following output −\n",
    "\n",
    "NN: noun, common, singular or mass\n",
    "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
    "    investment slide humour falloff slick wind hyena override subhumanity\n",
    "    machinist ...\n",
    "IN: preposition or conjunction, subordinating\n",
    "    astride among uppon whether out inside pro despite on by throughout\n",
    "    below within for towards near behind atop around if like until below\n",
    "    next into if beside ...\n",
    "DT: determiner\n",
    "    all an another any both del each either every half la many much nary\n",
    "    neither no some such that the them these this those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67859143-c920-4a2a-a2be-ac9a03140575",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727abf5-3dc8-4c71-91b4-f7e3e20eab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Novel Analytics.xlsx') as writer:\n",
    "    df.to_excel(writer, sheet_name='Victorian Age', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4fd6a9-e073-4daa-86d8-92efaada9527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If I get IOPub data rate exceed. Write this code in console - jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10 \n",
    "\n",
    "# list to store files\n",
    "res = []\n",
    "dir_path = r'D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Modern Age'\n",
    "tempText = ()\n",
    "i = 0\n",
    "a = \"\"\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        location = os.path.join(dir_path, path)\n",
    "        print(location)\n",
    "                \n",
    "        #Read file from location\n",
    "        Pass_Location(location) \n",
    "        #print(lines)\n",
    "        print(\"Columns of text converted to txt\")\n",
    "        \n",
    "        # Split the text into sentences using regular expressions\n",
    "        sentences = re.split('[.!?]',lines)\n",
    "\n",
    "        # Remove any empty sentences\n",
    "        sentences = list(filter(None, sentences))\n",
    "\n",
    "        # Calculate the average number of words per sentence\n",
    "        total_words = sum(len(sentence.split()) for sentence in sentences)\n",
    "        avg_words_per_sentence = total_words / len(sentences)\n",
    "\n",
    "        # Print the result\n",
    "        print(\"Average words per sentence:\", avg_words_per_sentence)\n",
    "        \n",
    "        \n",
    "        #Tokenization Sentence & word\n",
    "        tokenized_text=sent_tokenize(lines)\n",
    "       # print(tokenized_text) - its working. Closed it to avoid excess printing\n",
    "        tokenized_word=word_tokenize(lines)\n",
    "        #print(tokenized_word) - its working. Closed it to avoid excess printing\n",
    "        \n",
    "        print(\"No of sentences\", len(tokenized_text))\n",
    "        print(\"No of words\", len(tokenized_word))\n",
    "        \n",
    "        filtered_sent=[]\n",
    "        for w in tokenized_word:\n",
    "            if w not in stop_words:\n",
    "                filtered_sent.append(w)\n",
    "        #print(\"Tokenized Sentence:\",tokenized_text)\n",
    "        #print(\"Filterd Sentence:\",filtered_sent)\n",
    "                \n",
    "        fdist = FreqDist(filtered_sent)\n",
    "        print(fdist)\n",
    "        #print(fdist.most_common(25)) - will open later when printing to file\n",
    "        \n",
    "        #Creating the summation\n",
    "        # Tokenize the text\n",
    "        tokens = nltk.word_tokenize(lines)\n",
    "\n",
    "        # Perform POS tagging\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "        # Initialize counters\n",
    "        counts = Counter(tag for word, tag in pos_tags)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"POS counts:\", counts)\n",
    "\n",
    "        #Sentiment_Analysis(lines)\n",
    "        from textblob import TextBlob\n",
    "\n",
    "        # Load the text to analyze\n",
    "        # Perform sentiment analysis\n",
    "        blob = TextBlob(lines)\n",
    "        sentiment = blob.sentiment\n",
    "\n",
    "        # Print the results\n",
    "        print(\"Sentiment polarity:\", sentiment.polarity)\n",
    "        print(\"Sentiment subjectivity:\", sentiment.subjectivity)\n",
    "        \n",
    "        \n",
    "        from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "          \n",
    "\n",
    "        # Perform sentiment analysis\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        scores = analyzer.polarity_scores(lines)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"Sentiment scores:\", scores)\n",
    "        \n",
    "               \n",
    "       \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63047786-0e47-4dd3-8b06-f5193df0143d",
   "metadata": {},
   "outputs": [],
   "source": [
    " df2.loc[len(df2.index)] = [location, lines, len(tokenized_text), len(tokenized_word), avg_words_per_sentence, counts, sentiment.polarity, sentiment.subjectivity, scores, fdist.most_common(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ba25a-2965-4f6d-8cfc-5c61707cb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f66be-d3e9-4dcb-916c-512a19ac3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing Excel file\n",
    "book = pd.ExcelWriter('D:\\\\OneDrive\\\\Creative Writing\\\\Published\\\\Titles\\\\0. Anthologies etc\\\\Novel Analytics.xlsx', engine='xlsxwriter')\n",
    "writer = book.book\n",
    "existing_sheets = writer.sheetnames\n",
    "\n",
    "# Write the new data to a new sheet in the Excel file\n",
    "df2.to_excel(book, sheet_name='Modern Age', index=False)\n",
    "\n",
    "# Save and close the Excel file\n",
    "book.save()\n",
    "book.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b5975-0dd3-4514-8daa-1647ebd629fc",
   "metadata": {},
   "source": [
    "CC coordinating conjunction \n",
    "CD cardinal digit \n",
    "DT determiner \n",
    "EX existential there (like: “there is” … think of it like “there exists”) \n",
    "FW foreign word \n",
    "IN preposition/subordinating conjunction \n",
    "JJ adjective – ‘big’ \n",
    "JJR adjective, comparative – ‘bigger’ \n",
    "JJS adjective, superlative – ‘biggest’ \n",
    "LS list marker 1) \n",
    "MD modal – could, will \n",
    "NN noun, singular ‘- desk’ \n",
    "NNS noun plural – ‘desks’ \n",
    "NNP proper noun, singular – ‘Harrison’ \n",
    "NNPS proper noun, plural – ‘Americans’ \n",
    "PDT predeterminer – ‘all the kids’ \n",
    "POS possessive ending parent’s \n",
    "PRP personal pronoun –  I, he, she \n",
    "PRP$ possessive pronoun – my, his, hers \n",
    "RB adverb – very, silently, \n",
    "RBR adverb, comparative – better \n",
    "RBS adverb, superlative – best \n",
    "RP particle – give up \n",
    "TO – to go ‘to’ the store. \n",
    "UH interjection – errrrrrrrm \n",
    "VB verb, base form – take \n",
    "VBD verb, past tense – took \n",
    "VBG verb, gerund/present participle – taking \n",
    "VBN verb, past participle – taken \n",
    "VBP verb, sing. present, non-3d – take \n",
    "VBZ verb, 3rd person sing. present – takes \n",
    "WDT wh-determiner – which \n",
    "WP wh-pronoun – who, what \n",
    "WP$ possessive wh-pronoun, eg- whose \n",
    "WRB wh-adverb, eg- where, when\n",
    "\n",
    "TextBlob returns polarity and subjectivity of a sentence. Polarity lies between [-1,1], -1 defines a negative sentiment and 1 defines a positive sentiment. Negation words reverse the polarity. TextBlob has semantic labels that help with fine-grained analysis. For example — emoticons, exclamation mark, emojis, etc. Subjectivity lies between [0,1]. Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
